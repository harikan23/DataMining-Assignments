{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-255-Adaboost_scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Builiding Ababoost model from scratch for toy dataset.\n",
        "\n",
        "The name AdaBoost stands for Adaptive Boosting, and it refers to a particular boosting algorithm in which we fit a sequence ofdecision trees with a single node and two leaves and weight their contribution to the final prediction. "
      ],
      "metadata": {
        "id": "T3imemXxDNc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a toy dataset"
      ],
      "metadata": {
        "id": "OpvjitOqDINr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UIHNy1jeErm"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def make_toy_dataset(n: int = 100, random_seed: int = None):\n",
        "    \"\"\" Generate a toy dataset for evaluating AdaBoost classifiers \"\"\"\n",
        "    \n",
        "    n_per_class = int(n/2)\n",
        "    \n",
        "    if random_seed:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    X, y = make_gaussian_quantiles(n_samples=n, n_features=2, n_classes=2)\n",
        "    \n",
        "    return X, y*2-1\n",
        "\n",
        "X, y = make_toy_dataset(n=10, random_seed=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoost:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stumps = None\n",
        "        self.stump_weights = None\n",
        "        self.errors = None\n",
        "        self.sample_weights = None\n",
        "\n",
        "    def _check_X_y(self, X, y):\n",
        "        \"\"\" Validate assumptions about format of input data\"\"\"\n",
        "        assert set(y) == {-1, 1}, 'Response variable must be Â±1'\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "EafhzLlBfEY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the model"
      ],
      "metadata": {
        "id": "T4IbDAAHCl5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def fit(self, X: np.ndarray, y: np.ndarray, iters: int):\n",
        "\n",
        "    X, y = self._check_X_y(X, y)\n",
        "    n = X.shape[0]\n",
        "    self.sample_weights = np.zeros(shape=(iters, n))\n",
        "    self.stumps = np.zeros(shape=iters, dtype=object)\n",
        "    self.stump_weights = np.zeros(shape=iters)\n",
        "    self.errors = np.zeros(shape=iters)\n",
        "    self.sample_weights[0] = np.ones(shape=n) / n\n",
        "    for t in range(iters):\n",
        "        curr_sample_weights = self.sample_weights[t]\n",
        "        stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
        "        stump = stump.fit(X, y, sample_weight=curr_sample_weights)\n",
        "        stump_pred = stump.predict(X)\n",
        "        err = curr_sample_weights[(stump_pred != y)].sum()# / n\n",
        "        stump_weight = np.log((1 - err) / err) / 2\n",
        "        new_sample_weights = (curr_sample_weights * np.exp(-stump_weight * y * stump_pred))\n",
        "        new_sample_weights /= new_sample_weights.sum()\n",
        "        if t+1 < iters:\n",
        "            self.sample_weights[t+1] = new_sample_weights\n",
        "\n",
        "        # save results of iteration\n",
        "        self.stumps[t] = stump\n",
        "        self.stump_weights[t] = stump_weight\n",
        "        self.errors[t] = err\n",
        "\n",
        "    return self"
      ],
      "metadata": {
        "id": "fChpQ04zfKbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the values"
      ],
      "metadata": {
        "id": "s2eY1OqBCowW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    \"\"\" Make predictions using already fitted model \"\"\"\n",
        "    stump_preds = np.array([stump.predict(X) for stump in self.stumps])\n",
        "    return np.sign(np.dot(self.stump_weights, stump_preds))"
      ],
      "metadata": {
        "id": "q82a0AM6fWyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test error for the model"
      ],
      "metadata": {
        "id": "txumBl9GCtq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "AdaBoost.fit = fit\n",
        "AdaBoost.predict = predict\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = AdaBoost().fit(X_train, y_train, iters=10)\n",
        "\n",
        "train_err = (clf.predict(X_test) != y_test).mean()\n",
        "\n",
        "print(\"mean square error\",mean_squared_error(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aqCVKEQfZP5",
        "outputId": "b2e67112-81f3-4a47-dff2-b6bc27faffcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean square error 2.0\n"
          ]
        }
      ]
    }
  ]
}